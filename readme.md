## Introduction
For the current coursework I’ve implemented agent based on MDP. It mainly relies on the principles of value iteration and maximum expected utility (MEU).
## Description
The main pattern that Pacman follows on his way to victory consists of building a representation of his world, where he stores the given rewards, which are based on the current state of the world, as well the utilities for each position, which are calculated, based on the rewards. After he got all the utilities calculated, using value iteration, he jumps to next bit, where he calculated the expected utilities for all his possible moves and chooses the one that seems best.
## Strategy
When it comes to the rewards, the main thing about them in this agent is that the reward for a ghost isn’t just one fixed value. There are five stages of rewards for scared ghosts and five stages of rewards for when ghosts are chasing pacman.When the program is started, there are couple global variables,representing the rewards each cell will bring according to what is there. There very first thing it does is checking the dimensions of the board depending on them, it creates a representation of the game world with the actual size, which from the program’s point of views is a 2D array of 2-element lists. In the first element of the list in each cell is stored the reward, whereas in the second one is stored the utility, when it gets calculated. Then the program starts filling the rewards, by initially setting them all to zero if it’s the first run of the game. Otherwise, here comes the first key point of the strategy of the agent I’ve built. If it’s not the first run of the game, which is checked by a flag, all cells which are known from the previous run to not be walls are set to the pre-set rewards for empty cell, unless there is a ghost in vicinity(2 or 3 steps away)(however this is not always the case, the program checks how big is the map and if its width is less than 8, this doesn’t count, as tests show better performance without this bit on a small layout), then those cells are set with the 2nd and 3rd grade reward for ghosts, so they’re even more repulsive for Pacman.
Then it’s time for setting rewards for the other cells. For walls and capsules, it’s straightforward, on the spaces we have a wall or a capsule, we just set the reward. But regarding food and ghosts, there are the next key points in the Pacman’s strategy. When it comes to ghosts, after analysing their position, the distance to Pacman is calculated and according to it a different reward is set. The other thing that is taken into account is the state of the ghost, if the ghost is scared, positive reward is applied and vice-versa. As mentioned above, there are 5 stages for both positive and negative rewards which are as follow(all distances are calculated via the Manhattan distance function):
Stage 1: If ghost is closer than 3 units
Stage 2: … 5 units
Stage 3: … 7 units
Stage 4: … 10 
Stage 5: All further ghosts
Furthermore, when it comes to food, even though there is only one reward for food cell, if there’s a ghost closer than 2 steps from the food, the cell gets reward as it is an empty cell., however that’s not always the case, like it is with the empty cells near ghosts for the small layout.Then it comes time for value iteration, which is actually a nested for loop inside a while loop, which goes through every single cell in the world representation and loops until it reaches a threshold of 30 iterations or until values stop changing. When the loop is inside a cell, what happens is, for every possible move from that cell, the probabilities of landing in all possible cells form there are multiplied by the utilities of those cells and stored into a temporary list, from which at the end the maximum is taken, and the Bellman update is applied. After all the utilities are finally calculated, then Pacman needs to make a choice. To help him 
make a choice, we’re using the MEU method. From the current location of Pacman, we’re identifying all possible transitions. After that, for every single one of them, we’re calculating the best expected utility from there on. To do this the Bellman equation comes to help, which is similar to the Bellman update, apart from the fact that it’s only executed once.
After we have the expected utilities for two moves in advance, the one that looks most promising is chosen and Pacman hopes he doesn’t slip and goes actually in the intended direction.